# ç·Šæ€¥ä¿®æ­£ï¼šservices/summarize.py
# ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã«ã—ã¦æ–°ã—ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§å®Ÿè¡Œ

import tempfile
import shutil
from pathlib import Path
from infra.pdf_fetcher import fetch_pdf
from infra.vector_store import get_vector_store
from domain.text_ops import load_and_split
from langchain_community.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain.prompts import PromptTemplate
import chromadb
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OpenAIEmbeddings
import os

def create_clean_vector_store():
    """æ¯å›æ–°ã—ã„ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆï¼ˆå¤ã„ãƒ‡ãƒ¼ã‚¿æ··å…¥ã‚’é˜²ãï¼‰"""
    # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§æ–°ã—ã„ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆ
    temp_dir = tempfile.mkdtemp(prefix="pubmed_vs_")
    
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        raise ValueError("OPENAI_API_KEYç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    
    embeddings = OpenAIEmbeddings(openai_api_key=api_key)
    vs = Chroma(persist_directory=temp_dir, embedding_function=embeddings)
    
    return vs, temp_dir

def summarize_text_fixed(text: str) -> str:
    """ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢å•é¡Œã‚’ä¿®æ­£ã—ãŸè¦ç´„æ©Ÿèƒ½"""
    try:
        if len(text.strip()) < 100:
            return "ãƒ†ã‚­ã‚¹ãƒˆãŒçŸ­ã™ãã¦è¦ç´„ã§ãã¾ã›ã‚“ã€‚"
        
        print("ğŸ”§ æ–°ã—ã„ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆä¸­...")
        
        # ã‚¯ãƒªãƒ¼ãƒ³ãªãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆ
        vs, temp_dir = create_clean_vector_store()
        
        try:
            # é‡è¦ã‚»ã‚¯ã‚·ãƒ§ãƒ³æŠ½å‡º
            important_sections = extract_key_sections(text)
            print(f"ğŸ“‹ æŠ½å‡ºã•ã‚ŒãŸã‚»ã‚¯ã‚·ãƒ§ãƒ³: {list(important_sections.keys())}")
            
            # æ”¹è‰¯ã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
            docs = text_to_documents_improved(text, important_sections)
            print(f"ğŸ“¦ ä½œæˆã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯æ•°: {len(docs)}")
            
            # æ–°ã—ã„ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«è¿½åŠ 
            vs.add_documents(docs)
            print("âœ… ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«æ–‡æ›¸ã‚’è¿½åŠ å®Œäº†")
            
            # RAGæ¤œç´¢è¨­å®š
            retriever = vs.as_retriever(
                search_type="similarity",  # ã¾ãšã‚·ãƒ³ãƒ—ãƒ«ã«
                search_kwargs={"k": 8}     # é©åº¦ãªæ•°ã®ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—
            )
            
            # æ¤œç´¢ãƒ†ã‚¹ãƒˆï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
            test_query = "éª¨ç²—ã—ã‚‡ã†ç—‡ ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹ çµæœ"
            test_docs = retriever.get_relevant_documents(test_query)
            print(f"ğŸ” æ¤œç´¢ãƒ†ã‚¹ãƒˆçµæœ: {len(test_docs)} æ–‡æ›¸å–å¾—")
            if test_docs:
                print(f"ğŸ¯ æœ€åˆã®æ–‡æ›¸: {test_docs[0].page_content[:100]}...")
            
            # LLMè¨­å®š
            llm = ChatOpenAI(
                model_name="gpt-4o-mini", 
                temperature=0.1,
                max_tokens=1200
            )
            
            # åŒ»å­¦è«–æ–‡å°‚ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            medical_prompt = PromptTemplate(
                template="""
ã‚ãªãŸã¯åŒ»å­¦è«–æ–‡ã®å°‚é–€è¦ç´„è€…ã§ã™ã€‚ä»¥ä¸‹ã®è«–æ–‡ã®é–¢é€£éƒ¨åˆ†ã‚’èª­ã‚“ã§ã€æ­£ç¢ºãª3ç‚¹è¦ç´„ã‚’æ—¥æœ¬èªã§ä½œæˆã—ã¦ãã ã•ã„ã€‚

è«–æ–‡ã®é–¢é€£å†…å®¹:
{context}

è¦ç´„è¦ä»¶ï¼š
1. ã€ç ”ç©¶èƒŒæ™¯ãƒ»ç›®çš„ã€‘ã“ã®ç ”ç©¶ã®èƒŒæ™¯ã¨ç›®çš„ã‚’1-2æ–‡ã§
2. ã€æ–¹æ³•ãƒ»ä¸»è¦çµæœã€‘ç ”ç©¶æ–¹æ³•ã¨é‡è¦ãªçµæœï¼ˆæ•°å€¤ãƒ‡ãƒ¼ã‚¿å«ã‚€ï¼‰ã‚’1-2æ–‡ã§  
3. ã€çµè«–ãƒ»è‡¨åºŠçš„æ„ç¾©ã€‘ç ”ç©¶ã‹ã‚‰å¾—ã‚‰ã‚ŒãŸçµè«–ã¨è‡¨åºŠçš„æ„ç¾©ã‚’1-2æ–‡ã§

é‡è¦ï¼š
- é–¢é€£å†…å®¹ã‹ã‚‰ç›´æ¥çš„ã«æƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹
- æ–‡çŒ®æ¤œç´¢æ‰‹é †ã§ã¯ãªãç ”ç©¶å†…å®¹ã‚’è¦ç´„ã™ã‚‹
- æ•°å€¤ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°å¿…ãšå«ã‚ã‚‹

è³ªå•: {question}

åŒ»å­¦è«–æ–‡è¦ç´„:""",
                input_variables=["context", "question"]
            )
            
            # RAGãƒã‚§ãƒ¼ãƒ³æ§‹ç¯‰
            chain = RetrievalQA.from_chain_type(
                llm=llm, 
                retriever=retriever,
                chain_type="stuff",
                chain_type_kwargs={"prompt": medical_prompt}
            )
            
            question = "ã“ã®åŒ»å­¦è«–æ–‡ã«ã¤ã„ã¦ã€ç ”ç©¶èƒŒæ™¯ãƒ»æ–¹æ³•çµæœãƒ»çµè«–ã®3ç‚¹ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚"
            print("ğŸ¤– LLMã«ã‚ˆã‚‹è¦ç´„ç”Ÿæˆä¸­...")
            answer = chain.run(question)
            
            return answer.strip()
            
        finally:
            # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            shutil.rmtree(temp_dir, ignore_errors=True)
            print("ğŸ§¹ ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—")
            
    except Exception as e:
        print(f"âŒ ä¿®æ­£è¦ç´„ã‚¨ãƒ©ãƒ¼: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šç›´æ¥LLMè¦ç´„
        return direct_llm_summary_simple(text)

def direct_llm_summary_simple(text: str) -> str:
    """RAGã‚’ä½¿ã‚ãªã„ã‚·ãƒ³ãƒ—ãƒ«LLMè¦ç´„ï¼ˆç·Šæ€¥ç”¨ï¼‰"""
    try:
        llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.1, max_tokens=1000)
        
        # é‡è¦éƒ¨åˆ†ã‚’æŠ½å‡º
        sections = extract_key_sections(text)
        
        # Abstract + Results + Conclusionã‚’å„ªå…ˆ
        key_content = ""
        if sections['abstract']:
            key_content += f"Abstract: {sections['abstract'][:1000]}\n\n"
        if sections['results']:
            key_content += f"Results: {sections['results'][:1000]}\n\n"  
        if sections['conclusion']:
            key_content += f"Conclusion: {sections['conclusion'][:1000]}\n\n"
        
        # é‡è¦éƒ¨åˆ†ãŒãªã„å ´åˆã¯å…¨æ–‡ã®ä¸€éƒ¨ã‚’ä½¿ç”¨
        if not key_content.strip():
            key_content = text[:3000]
        
        prompt = f"""
ä»¥ä¸‹ã®åŒ»å­¦è«–æ–‡ã®é‡è¦éƒ¨åˆ†ã‚’èª­ã‚“ã§ã€3ç‚¹ã§æ­£ç¢ºã«è¦ç´„ã—ã¦ãã ã•ã„ï¼š

{key_content}

è¦ç´„å½¢å¼ï¼š
1. ã€ç ”ç©¶èƒŒæ™¯ãƒ»ç›®çš„ã€‘
2. ã€æ–¹æ³•ãƒ»ä¸»è¦çµæœã€‘ï¼ˆå…·ä½“çš„ãªæ•°å€¤ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€ï¼‰
3. ã€çµè«–ãƒ»è‡¨åºŠçš„æ„ç¾©ã€‘

å„ç‚¹ã¯1-2æ–‡ã§ç°¡æ½”ã«ã€‚é‡è¦ãªæ•°å€¤ã¯å¿…ãšå«ã‚ã¦ãã ã•ã„ã€‚

è¦ç´„ï¼š
"""
        
        response = llm.invoke(prompt)
        return response.content.strip()
        
    except Exception as e:
        return f"è¦ç´„ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}"

# extract_key_sections ã¨ text_to_documents_improved é–¢æ•°ã¯å‰å›ã¨åŒã˜ã‚‚ã®ã‚’ä½¿ç”¨

def extract_key_sections(text: str) -> dict:
    """é‡è¦ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆAbstract, Results, Conclusionï¼‰ã‚’æŠ½å‡º"""
    sections = {
        'abstract': '',
        'results': '',
        'conclusion': '',
        'methods': ''
    }
    
    lines = text.split('\n')
    current_section = None
    
    for line in lines:
        line_lower = line.lower().strip()
        
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¤å®šï¼ˆã‚ˆã‚Šæ­£ç¢ºã«ï¼‰
        if (any(word in line_lower for word in ['abstract', 'summary']) and 
            len(line) < 50 and 
            not line_lower.startswith('background')):
            current_section = 'abstract'
            continue
        elif (any(word in line_lower for word in ['result', 'finding']) and 
              len(line) < 50):
            current_section = 'results'
            continue
        elif (any(word in line_lower for word in ['conclusion', 'discussion']) and 
              len(line) < 50):
            current_section = 'conclusion'
            continue
        elif (any(word in line_lower for word in ['method', 'material']) and 
              len(line) < 50):
            current_section = 'methods'
            continue
        elif line_lower.startswith('##') or (line_lower.startswith('#') and len(line) < 100):
            # æ–°ã—ã„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒå§‹ã¾ã£ãŸå¯èƒ½æ€§
            current_section = None
            continue
            
        # å†…å®¹ã‚’è©²å½“ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«è¿½åŠ 
        if current_section and line.strip() and len(line.strip()) > 10:
            sections[current_section] += line + ' '
    
    return sections

def text_to_documents_improved(text: str, key_sections: dict) -> list[Document]:
    """åŒ»å­¦è«–æ–‡ç”¨æ”¹è‰¯ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²"""
    docs = []
    
    # é‡è¦ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å„ªå…ˆçš„ã«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåŒ–
    for section_name, section_text in key_sections.items():
        if section_text.strip() and len(section_text.strip()) > 50:
            weight = {
                'abstract': 3.0,
                'conclusion': 3.0, 
                'results': 2.5,
                'methods': 1.5
            }.get(section_name, 1.0)
            
            doc = Document(
                page_content=section_text.strip(),
                metadata={
                    "section": section_name,
                    "importance": weight,
                    "source": "key_section"
                }
            )
            docs.append(doc)
    
    # é€šå¸¸ã®ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã‚‚ä½µç”¨ï¼ˆé‡è¤‡é™¤å»ï¼‰
    used_content = set()
    for doc in docs:
        used_content.add(doc.page_content[:100])  # é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨
    
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=100,
        separators=["\n\n## ", "\n\n", "\n", "ã€‚", ". ", " ", ""]
    )
    
    chunks = splitter.split_text(text)
    
    for i, chunk in enumerate(chunks):
        chunk_preview = chunk[:100]
        if (len(chunk.strip()) > 50 and 
            chunk_preview not in used_content and
            not any(preview in chunk_preview for preview in used_content)):
            
            doc = Document(
                page_content=chunk,
                metadata={
                    "chunk_id": i,
                    "source": "general_chunk",
                    "importance": 1.0
                }
            )
            docs.append(doc)
            used_content.add(chunk_preview)
    
    return docs

# æ—¢å­˜é–¢æ•°ã®ç½®ãæ›ãˆ
def summarize_text(text: str) -> str:
    """ãƒ¡ã‚¤ãƒ³è¦ç´„é–¢æ•°"""
    return summarize_text_fixed(text)

def summarize_pdf(pdf_path: str) -> str:
    """PDFè¦ç´„ã‚‚ä¿®æ­£ç‰ˆã‚’ä½¿ç”¨"""
    docs = load_and_split(Path(pdf_path))
    full_text = "\n".join([doc.page_content for doc in docs])
    return summarize_text_fixed(full_text)