# ç·Šæ€¥ä¿®æ­£ï¼šservices/summarize.py
# ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã«ã—ã¦æ–°ã—ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§å®Ÿè¡Œ

import tempfile
import shutil
from pathlib import Path
from infra.pdf_fetcher import fetch_pdf
from infra.vector_store import get_vector_store
from domain.text_ops import load_and_split
from langchain_community.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain.prompts import PromptTemplate
import chromadb
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OpenAIEmbeddings
import os

def create_clean_vector_store():
    """æ¯å›æ–°ã—ã„ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆï¼ˆå¤ã„ãƒ‡ãƒ¼ã‚¿æ··å…¥ã‚’é˜²ãï¼‰"""
    # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§æ–°ã—ã„ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆ
    temp_dir = tempfile.mkdtemp(prefix="pubmed_vs_")
    
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        raise ValueError("OPENAI_API_KEYç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    
    embeddings = OpenAIEmbeddings(openai_api_key=api_key)
    vs = Chroma(persist_directory=temp_dir, embedding_function=embeddings)
    
    return vs, temp_dir

def summarize_text_fixed(text: str) -> str:
    """ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢å•é¡Œã‚’ä¿®æ­£ã—ãŸè¦ç´„æ©Ÿèƒ½"""
    try:
        if len(text.strip()) < 100:
            return "ãƒ†ã‚­ã‚¹ãƒˆãŒçŸ­ã™ãã¦è¦ç´„ã§ãã¾ã›ã‚“ã€‚"
        
        print("ğŸ”§ æ–°ã—ã„ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆä¸­...")
        
        # ã‚¯ãƒªãƒ¼ãƒ³ãªãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆ
        vs, temp_dir = create_clean_vector_store()
        
        try:
            # é‡è¦ã‚»ã‚¯ã‚·ãƒ§ãƒ³æŠ½å‡º
            important_sections = extract_key_sections(text)
            print(f"ğŸ“‹ æŠ½å‡ºã•ã‚ŒãŸã‚»ã‚¯ã‚·ãƒ§ãƒ³: {list(important_sections.keys())}")
            
            # æ”¹è‰¯ã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
            docs = text_to_documents_improved(text, important_sections)
            print(f"ğŸ“¦ ä½œæˆã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯æ•°: {len(docs)}")
            
            # æ–°ã—ã„ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«è¿½åŠ 
            vs.add_documents(docs)
            print("âœ… ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«æ–‡æ›¸ã‚’è¿½åŠ å®Œäº†")
            
            # RAGæ¤œç´¢è¨­å®š
            retriever = vs.as_retriever(
                search_type="similarity",  # ã¾ãšã‚·ãƒ³ãƒ—ãƒ«ã«
                search_kwargs={"k": 12}     # é©åº¦ãªæ•°ã®ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—
            )
            
            # æ¤œç´¢ãƒ†ã‚¹ãƒˆï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
            test_query = "éª¨ç²—ã—ã‚‡ã†ç—‡ ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹ çµæœ"
            test_docs = retriever.get_relevant_documents(test_query)
            print(f"ğŸ” æ¤œç´¢ãƒ†ã‚¹ãƒˆçµæœ: {len(test_docs)} æ–‡æ›¸å–å¾—")
            if test_docs:
                print(f"ğŸ¯ æœ€åˆã®æ–‡æ›¸: {test_docs[0].page_content[:100]}...")
            
            # LLMè¨­å®š
            llm = ChatOpenAI(
                model_name="gpt-4o-mini", 
                temperature=0.1,
                max_tokens=2500
            )
            
            # åŒ»å­¦è«–æ–‡å°‚ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            medical_prompt = PromptTemplate(
                template="""
ã‚ãªãŸã¯åŒ»å­¦è«–æ–‡ã®å°‚é–€è¦ç´„è€…ã§ã™ã€‚ä»¥ä¸‹ã®è«–æ–‡ã®é–¢é€£éƒ¨åˆ†ã‚’èª­ã‚“ã§ã€æ­£ç¢ºãª3ç‚¹è¦ç´„ã‚’æ—¥æœ¬èªã§ä½œæˆã—ã¦ãã ã•ã„ã€‚

è«–æ–‡ã®é–¢é€£å†…å®¹:
{context}

è¦ç´„è¦ä»¶ï¼š
1. ã€ç ”ç©¶èƒŒæ™¯ãƒ»ç›®çš„ã€‘
   - ç ”ç©¶å¯¾è±¡ç–¾æ‚£ã®é‡è¦æ€§ã¨æ—¢å­˜æ²»ç™‚ã®èª²é¡Œï¼ˆ2-3æ–‡ï¼‰
   - ã“ã®ç ”ç©¶ã®å…·ä½“çš„ãªç›®çš„ã¨ä»®èª¬ï¼ˆ1-2æ–‡ï¼‰

2. ã€æ–¹æ³•ãƒ»ä¸»è¦çµæœã€‘
   - ç ”ç©¶ãƒ‡ã‚¶ã‚¤ãƒ³ã€å¯¾è±¡è€…æ•°ã€æœŸé–“ã€è¨­å®šï¼ˆ1-2æ–‡ï¼‰
   - é‡è¦ãªçµæœã‚’å…·ä½“çš„ãªæ•°å€¤ã¨ã¨ã‚‚ã«è©³è¿°ï¼ˆ3-4æ–‡ï¼‰
   - çµ±è¨ˆçš„æœ‰æ„æ€§ã‚„ä¿¡é ¼åŒºé–“ãŒã‚ã‚Œã°å«ã‚ã‚‹

3. ã€çµè«–ãƒ»è‡¨åºŠçš„æ„ç¾©ã€‘
   - ç ”ç©¶ã‹ã‚‰å¾—ã‚‰ã‚ŒãŸä¸»è¦ãªçŸ¥è¦‹ï¼ˆ2-3æ–‡ï¼‰
   - è‡¨åºŠç¾å ´ã¸ã®å…·ä½“çš„ãªå½±éŸ¿ã¨æ¨å¥¨äº‹é …ï¼ˆ1-2æ–‡ï¼‰
   - ç ”ç©¶ã®é™ç•Œã¨ä»Šå¾Œã®å±•æœ›ï¼ˆ1æ–‡ï¼‰

é‡è¦äº‹é …ï¼š
- å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆè¨ˆ150-250æ–‡å­—ã‚’ç›®å®‰ã«è©³ã—ãè¨˜è¿°
- é‡è¦ãªæ•°å€¤ãƒ‡ãƒ¼ã‚¿ã€ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ã€ä¿¡é ¼åŒºé–“ã¯å¿…ãšå«ã‚ã‚‹
- åŒ»å­¦çš„å°‚é–€ç”¨èªã‚’é©åˆ‡ã«ä½¿ç”¨ã—ã€ä¿¡é ¼æ€§ã®é«˜ã„è¦ç´„ã¨ã™ã‚‹
- çµæœã®è‡¨åºŠçš„æ„ç¾©ã‚’å…·ä½“çš„ã«èª¬æ˜ã™ã‚‹

è³ªå•: {question}

åŒ»å­¦è«–æ–‡è¦ç´„:""",
                input_variables=["context", "question"]
            )
            
            # RAGãƒã‚§ãƒ¼ãƒ³æ§‹ç¯‰
            chain = RetrievalQA.from_chain_type(
                llm=llm, 
                retriever=retriever,
                chain_type="stuff",
                chain_type_kwargs={"prompt": medical_prompt}
            )
            
            question = "ã“ã®åŒ»å­¦è«–æ–‡ã«ã¤ã„ã¦ã€ç ”ç©¶èƒŒæ™¯ãƒ»æ–¹æ³•çµæœãƒ»çµè«–ã®3ç‚¹ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚"
            print("ğŸ¤– LLMã«ã‚ˆã‚‹è¦ç´„ç”Ÿæˆä¸­...")
            answer = chain.run(question)
            
            return answer.strip()
            
        finally:
            # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            shutil.rmtree(temp_dir, ignore_errors=True)
            print("ğŸ§¹ ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—")
            
    except Exception as e:
        print(f"âŒ ä¿®æ­£è¦ç´„ã‚¨ãƒ©ãƒ¼: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šç›´æ¥LLMè¦ç´„
        return direct_llm_summary_simple(text)

def text_to_documents_improved_extended(text: str, key_sections: dict) -> list[Document]:
    """ã‚ˆã‚Šå¤šãã®æƒ…å ±ã‚’å«ã‚€ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²"""
    docs = []
    
    # é‡è¦ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å„ªå…ˆçš„ã«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåŒ–ï¼ˆæ–‡å­—æ•°åˆ¶é™ã‚’ç·©å’Œï¼‰
    for section_name, section_text in key_sections.items():
        if section_text.strip() and len(section_text.strip()) > 50:
            weight = {
                'abstract': 3.0,
                'conclusion': 3.0, 
                'results': 2.5,
                'methods': 2.0  # Methodsã®é‡è¦åº¦ã‚‚ä¸Šã’ã‚‹
            }.get(section_name, 1.0)
            
            # é•·ã„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯åˆ†å‰²ã—ã¦è¤‡æ•°ã®ãƒãƒ£ãƒ³ã‚¯ã«
            if len(section_text) > 1500:
                # é•·ã„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¤‡æ•°ã«åˆ†å‰²
                splitter = RecursiveCharacterTextSplitter(
                    chunk_size=1500,
                    chunk_overlap=200
                )
                sub_chunks = splitter.split_text(section_text)
                
                for i, sub_chunk in enumerate(sub_chunks):
                    doc = Document(
                        page_content=sub_chunk.strip(),
                        metadata={
                            "section": f"{section_name}_part_{i+1}",
                            "importance": weight,
                            "source": "key_section_split"
                        }
                    )
                    docs.append(doc)
            else:
                doc = Document(
                    page_content=section_text.strip(),
                    metadata={
                        "section": section_name,
                        "importance": weight,
                        "source": "key_section"
                    }
                )
                docs.append(doc)
    
    # é€šå¸¸ã®ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã‚‚ä½µç”¨ï¼ˆãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã‚’å¢—åŠ ï¼‰
    used_content = set()
    for doc in docs:
        used_content.add(doc.page_content[:100])
    
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1500,  # 1000 â†’ 1500 ã«å¢—åŠ 
        chunk_overlap=200,  # 100 â†’ 200 ã«å¢—åŠ 
        separators=["\n\n## ", "\n\n", "\n", "ã€‚", ". ", " ", ""]
    )
    
    chunks = splitter.split_text(text)
    
    for i, chunk in enumerate(chunks):
        chunk_preview = chunk[:100]
        if (len(chunk.strip()) > 50 and 
            chunk_preview not in used_content and
            not any(preview in chunk_preview for preview in used_content)):
            
            doc = Document(
                page_content=chunk,
                metadata={
                    "chunk_id": i,
                    "source": "general_chunk",
                    "importance": 1.0
                }
            )
            docs.append(doc)
            used_content.add(chunk_preview)
    
    return docs

def direct_llm_summary_detailed(text: str) -> str:
    """RAGã‚’ä½¿ã‚ãªã„è©³ç´°LLMè¦ç´„ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
    try:
        llm = ChatOpenAI(
            model_name="gpt-4o-mini", 
            temperature=0.1, 
            max_tokens=2500  # å¤§å¹…ã«å¢—åŠ 
        )
        
        # é‡è¦éƒ¨åˆ†ã‚’æŠ½å‡º
        sections = extract_key_sections(text)
        
        # ã‚ˆã‚Šå¤šãã®å†…å®¹ã‚’å«ã‚ã‚‹
        key_content = ""
        if sections['abstract']:
            key_content += f"Abstract: {sections['abstract'][:2000]}\n\n"
        if sections['results']:
            key_content += f"Results: {sections['results'][:2000]}\n\n"  
        if sections['conclusion']:
            key_content += f"Conclusion: {sections['conclusion'][:2000]}\n\n"
        if sections['methods']:
            key_content += f"Methods: {sections['methods'][:1500]}\n\n"
        
        # é‡è¦éƒ¨åˆ†ãŒãªã„å ´åˆã¯å…¨æ–‡ã®ã‚ˆã‚Šå¤šãã®éƒ¨åˆ†ã‚’ä½¿ç”¨
        if not key_content.strip():
            key_content = text[:6000]  # 3000 â†’ 6000 ã«å¢—åŠ 
        
        prompt = f"""
ä»¥ä¸‹ã®åŒ»å­¦è«–æ–‡ã‚’èª­ã‚“ã§ã€è©³ç´°ã§æœ‰ç”¨ãª3ç‚¹è¦ç´„ã‚’æ—¥æœ¬èªã§ä½œæˆã—ã¦ãã ã•ã„ï¼š

{key_content}

è¦ç´„è¦ä»¶ï¼š
1. ã€ç ”ç©¶èƒŒæ™¯ãƒ»ç›®çš„ã€‘ï¼ˆ150-200æ–‡å­—ç¨‹åº¦ï¼‰
   - å¯¾è±¡ç–¾æ‚£ã®é‡è¦æ€§ã¨æ—¢å­˜æ²»ç™‚ã®èª²é¡Œã‚’è©³è¿°
   - æœ¬ç ”ç©¶ã®å…·ä½“çš„ãªç›®çš„ã¨æ„ç¾©ã‚’æ˜ç¢ºã«

2. ã€æ–¹æ³•ãƒ»ä¸»è¦çµæœã€‘ï¼ˆ200-300æ–‡å­—ç¨‹åº¦ï¼‰
   - ç ”ç©¶ãƒ‡ã‚¶ã‚¤ãƒ³ã€å¯¾è±¡è€…ã€æœŸé–“ã‚’å…·ä½“çš„ã«
   - é‡è¦ãªçµæœã‚’æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã¨ã¨ã‚‚ã«è©³ç´°ã«è¨˜è¿°
   - çµ±è¨ˆçš„æœ‰æ„æ€§ã‚„ä¿¡é ¼åŒºé–“ãŒã‚ã‚Œã°å«ã‚ã‚‹

3. ã€çµè«–ãƒ»è‡¨åºŠçš„æ„ç¾©ã€‘ï¼ˆ150-200æ–‡å­—ç¨‹åº¦ï¼‰
   - ç ”ç©¶ã®ä¸»è¦ãªçŸ¥è¦‹ã¨è‡¨åºŠã¸ã®å…·ä½“çš„å½±éŸ¿
   - ç ”ç©¶ã®é™ç•Œã¨ä»Šå¾Œã®å±•æœ›ã‚‚å«ã‚ã‚‹

æ³¨æ„ï¼š
- å„ç‚¹ã‚’ã—ã£ã‹ã‚Šã¨ã—ãŸåˆ†é‡ã§è¨˜è¿°ã™ã‚‹
- æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã¯å¿…ãšå«ã‚ã‚‹
- åŒ»å­¦çš„ã«æ­£ç¢ºã§è©³ç´°ãªè¡¨ç¾ã‚’ä½¿ç”¨

è©³ç´°è¦ç´„ï¼š
"""
        
        response = llm.invoke(prompt)
        return response.content.strip()
        
    except Exception as e:
        return f"è©³ç´°è¦ç´„ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}"

# extract_key_sections ã¨ text_to_documents_improved é–¢æ•°ã¯å‰å›ã¨åŒã˜ã‚‚ã®ã‚’ä½¿ç”¨

def extract_key_sections(text: str) -> dict:
    """é‡è¦ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆAbstract, Results, Conclusionï¼‰ã‚’æŠ½å‡º"""
    sections = {
        'abstract': '',
        'results': '',
        'conclusion': '',
        'methods': ''
    }
    
    lines = text.split('\n')
    current_section = None
    
    for line in lines:
        line_lower = line.lower().strip()
        
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¤å®šï¼ˆã‚ˆã‚Šæ­£ç¢ºã«ï¼‰
        if (any(word in line_lower for word in ['abstract', 'summary']) and 
            len(line) < 50 and 
            not line_lower.startswith('background')):
            current_section = 'abstract'
            continue
        elif (any(word in line_lower for word in ['result', 'finding']) and 
              len(line) < 50):
            current_section = 'results'
            continue
        elif (any(word in line_lower for word in ['conclusion', 'discussion']) and 
              len(line) < 50):
            current_section = 'conclusion'
            continue
        elif (any(word in line_lower for word in ['method', 'material']) and 
              len(line) < 50):
            current_section = 'methods'
            continue
        elif line_lower.startswith('##') or (line_lower.startswith('#') and len(line) < 100):
            # æ–°ã—ã„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒå§‹ã¾ã£ãŸå¯èƒ½æ€§
            current_section = None
            continue
            
        # å†…å®¹ã‚’è©²å½“ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«è¿½åŠ 
        if current_section and line.strip() and len(line.strip()) > 10:
            sections[current_section] += line + ' '
    
    return sections

def text_to_documents_improved(text: str, key_sections: dict) -> list[Document]:
    """åŒ»å­¦è«–æ–‡ç”¨æ”¹è‰¯ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²"""
    docs = []
    
    # é‡è¦ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å„ªå…ˆçš„ã«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåŒ–
    for section_name, section_text in key_sections.items():
        if section_text.strip() and len(section_text.strip()) > 50:
            weight = {
                'abstract': 3.0,
                'conclusion': 3.0, 
                'results': 2.5,
                'methods': 1.5
            }.get(section_name, 1.0)
            
            doc = Document(
                page_content=section_text.strip(),
                metadata={
                    "section": section_name,
                    "importance": weight,
                    "source": "key_section"
                }
            )
            docs.append(doc)
    
    # é€šå¸¸ã®ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã‚‚ä½µç”¨ï¼ˆé‡è¤‡é™¤å»ï¼‰
    used_content = set()
    for doc in docs:
        used_content.add(doc.page_content[:100])  # é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨
    
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=100,
        separators=["\n\n## ", "\n\n", "\n", "ã€‚", ". ", " ", ""]
    )
    
    chunks = splitter.split_text(text)
    
    for i, chunk in enumerate(chunks):
        chunk_preview = chunk[:100]
        if (len(chunk.strip()) > 50 and 
            chunk_preview not in used_content and
            not any(preview in chunk_preview for preview in used_content)):
            
            doc = Document(
                page_content=chunk,
                metadata={
                    "chunk_id": i,
                    "source": "general_chunk",
                    "importance": 1.0
                }
            )
            docs.append(doc)
            used_content.add(chunk_preview)
    
    return docs

# æ—¢å­˜é–¢æ•°ã®ç½®ãæ›ãˆ
def summarize_text(text: str) -> str:
    """ãƒ¡ã‚¤ãƒ³è¦ç´„é–¢æ•°(è©³ç´°ç‰ˆ)"""
    return summarize_text_fixed(text)

def summarize_pdf(pdf_path: str) -> str:
    """PDFè¦ç´„ã‚‚ä¿®æ­£ç‰ˆã‚’ä½¿ç”¨"""
    docs = load_and_split(Path(pdf_path))
    full_text = "\n".join([doc.page_content for doc in docs])
    return summarize_text_fixed(full_text)