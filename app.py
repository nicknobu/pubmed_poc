# -------------------------------
# file: app.py (PMC HTML/XMLæœ¬æ–‡æŠ½å‡ºæ©Ÿèƒ½ä»˜ã)
# -------------------------------
"""Streamlit UI â€• URL/DOI ã‚’å…¥åŠ›ã™ã‚‹ã¨ 3 ç‚¹è¦ç´„ã‚’è¡¨ç¤ºï¼ˆPDF + HTMLå¯¾å¿œï¼‰"""
import os
import re
import tempfile
import requests
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import streamlit as st
from services.summarize import summarize_pdf, summarize_text
from dotenv import load_dotenv
load_dotenv()

import openai  # å¿…è¦ã«å¿œã˜ã¦å…ˆé ­ã§ import

# .env èª­ã¿è¾¼ã¿æ¸ˆã¿ã®å‰æ
openai_key = os.environ.get("OPENAI_API_KEY")

if not openai_key:
    raise ValueError("ç’°å¢ƒå¤‰æ•° OPENAI_API_KEY ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚Hugging Face ã® Secrets ã¾ãŸã¯ .env ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

openai.api_key = openai_key

def extract_pmc_html_content(pmcid):
    """PMCè¨˜äº‹ã®HTMLãƒšãƒ¼ã‚¸ã‹ã‚‰æœ¬æ–‡ã‚’æŠ½å‡ºã™ã‚‹"""
    try:
        # PMCIDã‚’æ­£è¦åŒ–
        if not pmcid.upper().startswith('PMC'):
            pmcid = f"PMC{pmcid}"
        
        pmc_number = pmcid.replace('PMC', '')
        html_url = f"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC{pmc_number}/"
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
        
        response = requests.get(html_url, headers=headers, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # PMCè¨˜äº‹ã®æœ¬æ–‡æ§‹é€ ã‚’è§£æ
        content_parts = []
        
        # ã‚¿ã‚¤ãƒˆãƒ«
        title_elem = soup.find('h1', class_='content-title') or soup.find('title')
        if title_elem:
            content_parts.append(f"Title: {title_elem.get_text(strip=True)}\n")
        
        # è‘—è€…æƒ…å ±
        authors_elem = soup.find('div', class_='contrib-group')
        if authors_elem:
            authors = [a.get_text(strip=True) for a in authors_elem.find_all('a')]
            if authors:
                content_parts.append(f"Authors: {', '.join(authors)}\n")
        
        # ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆ
        abstract_elem = soup.find('div', {'class': 'abstract'}) or soup.find('div', id='abstract')
        if abstract_elem:
            abstract_text = abstract_elem.get_text(separator=' ', strip=True)
            content_parts.append(f"Abstract:\n{abstract_text}\n\n")
        
        # ãƒ¡ã‚¤ãƒ³æœ¬æ–‡ - è¤‡æ•°ã®ã‚»ãƒ¬ã‚¯ã‚¿ã‚’è©¦è¡Œ
        main_content = None
        
        # PMCã®ä¸€èˆ¬çš„ãªæœ¬æ–‡ã‚»ãƒ¬ã‚¯ã‚¿
        content_selectors = [
            'div.article-content',
            'div.fulltext-view',
            'div.tsec',
            'article',
            'div.content',
            'main'
        ]
        
        for selector in content_selectors:
            elements = soup.select(selector)
            if elements:
                # æœ€å¤§ã®è¦ç´ ã‚’é¸æŠ
                main_content = max(elements, key=lambda x: len(x.get_text()))
                break
        
        if main_content:
            # ä¸è¦ãªè¦ç´ ã‚’é™¤å»
            for unwanted in main_content.find_all(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                unwanted.decompose()
            
            # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã”ã¨ã«æ•´ç†
            sections = main_content.find_all(['h1', 'h2', 'h3', 'p', 'div'])
            
            for section in sections:
                text = section.get_text(separator=' ', strip=True)
                if text and len(text) > 20:  # çŸ­ã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆã¯é™¤å¤–
                    # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚¿ã‚°ã®å ´åˆ
                    if section.name in ['h1', 'h2', 'h3']:
                        content_parts.append(f"\n## {text}\n")
                    else:
                        content_parts.append(f"{text}\n")
        
        # çµè«–ãƒ»ã¾ã¨ã‚
        conclusion_elem = soup.find('div', class_='sec') or soup.find('section', id='conclusion')
        if conclusion_elem and 'conclusion' in conclusion_elem.get_text().lower():
            conclusion_text = conclusion_elem.get_text(separator=' ', strip=True)
            content_parts.append(f"\nConclusion:\n{conclusion_text}\n")
        
        full_text = '\n'.join(content_parts)
        
        # æœ€å°é™ã®æ–‡å­—æ•°ãƒã‚§ãƒƒã‚¯
        if len(full_text.strip()) < 200:
            st.warning("æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆãŒçŸ­ã™ãã¾ã™ã€‚PDFç‰ˆã®åˆ©ç”¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")
            return None
        
        return full_text.strip()
        
    except Exception as e:
        st.error(f"PMC HTMLæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        return None

def extract_pmc_xml_content(pmcid):
    """PMCè¨˜äº‹ã®XMLã‹ã‚‰æœ¬æ–‡ã‚’æŠ½å‡ºã™ã‚‹ï¼ˆæ›´ã«é«˜ç²¾åº¦ï¼‰"""
    try:
        # PMCIDã‚’æ­£è¦åŒ–
        if not pmcid.upper().startswith('PMC'):
            pmcid = f"PMC{pmcid}"
        
        # PMC OA APIã‚’ä½¿ç”¨ã—ã¦XMLã‚’å–å¾—
        xml_url = f"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi"
        params = {
            'db': 'pmc',
            'id': pmcid,
            'retmode': 'xml',
            'rettype': 'full'
        }
        
        response = requests.get(xml_url, params=params, timeout=15)
        response.raise_for_status()
        
        # XMLã‚’ãƒ‘ãƒ¼ã‚¹
        root = ET.fromstring(response.text)
        
        content_parts = []
        
        # ã‚¿ã‚¤ãƒˆãƒ«
        title_elem = root.find('.//article-title')
        if title_elem is not None:
            content_parts.append(f"Title: {title_elem.text}\n")
        
        # ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆ
        abstract_elem = root.find('.//abstract')
        if abstract_elem is not None:
            abstract_text = ' '.join(abstract_elem.itertext())
            content_parts.append(f"Abstract:\n{abstract_text}\n\n")
        
        # æœ¬æ–‡ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        body = root.find('.//body')
        if body is not None:
            for sec in body.findall('.//sec'):
                # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒˆãƒ«
                title_elem = sec.find('title')
                if title_elem is not None:
                    content_parts.append(f"\n## {title_elem.text}\n")
                
                # ã‚»ã‚¯ã‚·ãƒ§ãƒ³å†…å®¹
                for p in sec.findall('.//p'):
                    p_text = ' '.join(p.itertext())
                    if p_text.strip():
                        content_parts.append(f"{p_text.strip()}\n")
        
        full_text = '\n'.join(content_parts)
        
        if len(full_text.strip()) < 200:
            st.warning("XMLã‹ã‚‰ååˆ†ãªãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
            return None
        
        return full_text.strip()
        
    except Exception as e:
        st.warning(f"PMC XMLæŠ½å‡ºè©¦è¡Œå¤±æ•—: {e}")
        return None

def check_pmc_pdf_availability(pmcid):
    """PMC OA Web Service APIã‚’ä½¿ã£ã¦PDFãŒåˆ©ç”¨å¯èƒ½ã‹ç¢ºèªã™ã‚‹"""
    try:
        if not pmcid.upper().startswith('PMC'):
            pmcid = f"PMC{pmcid}"
        
        api_url = "https://www.ncbi.nlm.nih.gov/pmc/utils/oa/oa.fcgi"
        params = {'id': pmcid}
        
        response = requests.get(api_url, params=params, timeout=10)
        response.raise_for_status()
        
        # XMLã‚’ãƒ‘ãƒ¼ã‚¹
        root = ET.fromstring(response.text)
        
        # PDFãƒªãƒ³ã‚¯ã‚’æ¢ã™
        pdf_links = []
        for record in root.findall('.//record'):
            for link in record.findall('link[@format="pdf"]'):
                href = link.get('href')
                if href:
                    pdf_links.append(href)
        
        if pdf_links:
            return True, pdf_links[0]
        else:
            return False, None
    
    except Exception as e:
        st.warning(f"PMC PDFç¢ºèªã§ã‚¨ãƒ©ãƒ¼: {e}")
        return None, None

def is_doi(text):
    """DOIå½¢å¼ã‹ã©ã†ã‹ã‚’åˆ¤å®šã™ã‚‹"""
    doi_pattern = r'^10\.\d+/.+'
    return bool(re.match(doi_pattern, text.strip()))

def is_pmcid(text):
    """PMCIDå½¢å¼ã‹ã©ã†ã‹ã‚’åˆ¤å®šã™ã‚‹"""
    pmcid_pattern = r'^PMC\d+$'
    return bool(re.match(pmcid_pattern, text.strip().upper()))

def is_pmid(text):
    """PMIDï¼ˆæ•°å­—ã®ã¿ï¼‰ã‹ã©ã†ã‹ã‚’åˆ¤å®šã™ã‚‹"""
    return text.strip().isdigit()

def is_pdf_content(response):
    """ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒPDFã‹ã©ã†ã‹ã‚’åˆ¤å®šã™ã‚‹"""
    content_type = response.headers.get('content-type', '').lower()
    if 'pdf' in content_type:
        return True
    
    content_start = response.content[:10]
    return content_start.startswith(b'%PDF-')

def get_pdf_from_url(url):
    """URLã‹ã‚‰PDFã‚’å–å¾—ã™ã‚‹"""
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
        
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        
        if is_pdf_content(response):
            return response.content
        else:
            return None
            
    except Exception as e:
        st.error(f"PDFå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def get_content_from_pmcid(pmcid):
    """PMCIDã‹ã‚‰æœ€é©ãªå½¢å¼ã§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ã™ã‚‹ï¼ˆPDFå„ªå…ˆã€HTML/XMLãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
    if not pmcid.upper().startswith('PMC'):
        pmcid = f"PMC{pmcid}"
    
    st.info(f"ğŸ“„ {pmcid} ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ä¸­...")
    
    # 1. PDFå­˜åœ¨ç¢ºèª
    has_pdf, ftp_pdf_url = check_pmc_pdf_availability(pmcid)
    
    if has_pdf:
        st.info("âœ… PDFç‰ˆãŒåˆ©ç”¨å¯èƒ½ã§ã™ã€‚PDFç‰ˆã‚’å–å¾—ã—ã¾ã™...")
        pdf_content = get_pdf_from_url(ftp_pdf_url)
        if pdf_content and pdf_content.startswith(b'%PDF-'):
            return 'pdf', pdf_content
        st.warning("PDFå–å¾—ã«å¤±æ•—ã€‚ãƒ†ã‚­ã‚¹ãƒˆç‰ˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™...")
    
    # 2. XMLç‰ˆã‚’è©¦è¡Œ
    st.info("ğŸ“„ XMLç‰ˆã‹ã‚‰æœ¬æ–‡ã‚’æŠ½å‡ºä¸­...")
    xml_text = extract_pmc_xml_content(pmcid)
    if xml_text:
        st.success("âœ… XMLç‰ˆã‹ã‚‰æœ¬æ–‡æŠ½å‡ºæˆåŠŸï¼")
        return 'text', xml_text
    
    # 3. HTMLç‰ˆã‚’è©¦è¡Œ
    st.info("ğŸŒ HTMLç‰ˆã‹ã‚‰æœ¬æ–‡ã‚’æŠ½å‡ºä¸­...")
    html_text = extract_pmc_html_content(pmcid)
    if html_text:
        st.success("âœ… HTMLç‰ˆã‹ã‚‰æœ¬æ–‡æŠ½å‡ºæˆåŠŸï¼")
        return 'text', html_text
    
    # 4. ã™ã¹ã¦å¤±æ•—
    st.error(f"âŒ {pmcid} ã‹ã‚‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
    return None, None

def get_pdf_from_pmid(pmid):
    """PMIDã‹ã‚‰PMCIDã‚’å–å¾—ã—ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ã™ã‚‹"""
    try:
        api_url = f"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi"
        params = {
            'dbfrom': 'pubmed',
            'db': 'pmc',
            'id': pmid,
            'retmode': 'xml'
        }
        
        response = requests.get(api_url, params=params, timeout=10)
        
        pmc_match = re.search(r'<Id>(\d+)</Id>', response.text)
        if pmc_match:
            pmc_id = pmc_match.group(1)
            pmcid = f"PMC{pmc_id}"
            st.info(f"Found PMCID: {pmcid}")
            return get_content_from_pmcid(pmcid)
        else:
            st.warning("ã“ã®PMIDã«å¯¾å¿œã™ã‚‹PMCè¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return None, None
        
    except Exception as e:
        st.error(f"PMIDè§£æ±ºã‚¨ãƒ©ãƒ¼: {e}")
        return None, None

def resolve_to_content(input_text):
    """å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’è§£æã—ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ã™ã‚‹"""
    input_text = input_text.strip()
    
    # æ—¢ã«URLã®å ´åˆ
    if input_text.startswith(('http://', 'https://')):
        st.info(f"URL detected: {input_text}")
        pdf_content = get_pdf_from_url(input_text)
        if pdf_content:
            return 'pdf', pdf_content
        return None, None
    
    # PMCIDã®å ´åˆ
    if is_pmcid(input_text):
        st.info(f"PMCID detected: {input_text}")
        return get_content_from_pmcid(input_text)
    
    # PMIDã®å ´åˆ
    if is_pmid(input_text):
        st.info(f"PMID detected: {input_text}")
        return get_pdf_from_pmid(input_text)
    
    # DOIã®å ´åˆï¼ˆç°¡ç•¥åŒ–ï¼‰
    if is_doi(input_text):
        st.info(f"DOI detected: {input_text}")
        st.warning("DOIå¯¾å¿œã¯ç°¡ç•¥å®Ÿè£…ã€‚ç›´æ¥PMCIDã¾ãŸã¯PMIDã®ä½¿ç”¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
        return None, None
    
    # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®å ´åˆ
    if os.path.exists(input_text):
        with open(input_text, 'rb') as f:
            content = f.read()
            if content.startswith(b'%PDF-'):
                return 'pdf', content
            else:
                return 'text', content.decode('utf-8', errors='ignore')
    
    st.error("å…¥åŠ›å½¢å¼ãŒèªè­˜ã§ãã¾ã›ã‚“ã€‚PMCIDã¾ãŸã¯PMIDã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
    return None, None

# Streamlit UI
st.set_page_config(page_title="PubMed Summarizer", page_icon="ğŸ§¬")
st.title("ğŸ§¬ PubMed 3 ç‚¹è¦ç´„ PoC (PDF + HTMLå¯¾å¿œ)")

# æ©Ÿèƒ½èª¬æ˜
st.success("ğŸ†• **æ–°æ©Ÿèƒ½**: PDFæœªæä¾›ã®è¨˜äº‹ã‚‚HTML/XMLã‹ã‚‰æœ¬æ–‡æŠ½å‡ºã—ã¦è¦ç´„å¯èƒ½ï¼")

st.markdown("""
**å¯¾å¿œã™ã‚‹å…¥åŠ›å½¢å¼ & å–å¾—æ–¹æ³•ï¼š**
- ğŸ“‹ **PMCID**: `PMC1234567` â†’ PDFå„ªå…ˆã€HTML/XMLãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
- ğŸ“„ **PDF URL**: ç›´æ¥PDFå–å¾—
""")

input_text = st.text_input("PMCIDã¾ãŸã¯PDFã®URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", placeholder="ä¾‹: PMC12085841 ã¾ãŸã¯ https://example.com/sample.pdf")


if st.button("è¦ç´„"):
    if not input_text:
        st.error("è­˜åˆ¥å­ã¾ãŸã¯URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    else:
        with st.spinner("ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ä¸­â€¦"):
            content_type, content = resolve_to_content(input_text)
            
            if not content:
                st.error("ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                st.stop()
        
        with st.spinner("è¦ç´„ç”Ÿæˆä¸­â€¦"):
            try:
                if content_type == 'pdf':
                    # PDFãƒ•ã‚¡ã‚¤ãƒ«ã®å¦¥å½“æ€§ã‚’ãƒã‚§ãƒƒã‚¯
                    if not content.startswith(b'%PDF-'):
                        st.error("å–å¾—ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒPDFå½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
                        st.stop()

                    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¦PDFè¦ç´„
                    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
                        tmp_file.write(content)
                        tmp_pdf_path = tmp_file.name

                    result = summarize_pdf(tmp_pdf_path)
                    os.unlink(tmp_pdf_path)
                
                elif content_type == 'text':
                    # ãƒ†ã‚­ã‚¹ãƒˆç›´æ¥è¦ç´„
                    result = summarize_text(content)
                
                st.success("ğŸ“‹ è¦ç´„çµæœï¼š")
                st.write(result)
                
                # å–å¾—æ–¹æ³•ã‚’è¡¨ç¤º
                st.info(f"ğŸ“Š å–å¾—æ–¹æ³•: {content_type.upper()}å½¢å¼")
                    
            except Exception as e:
                import traceback
                st.error(f"è¦ç´„å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                st.text(traceback.format_exc())
# ãƒ‡ãƒãƒƒã‚°æ©Ÿèƒ½è¿½åŠ ï¼ˆè¦ç´„å“è³ªæ”¹å–„ç”¨ï¼‰
st.markdown("---")
st.subheader("ğŸ”§ ãƒ‡ãƒãƒƒã‚°æ©Ÿèƒ½")

if st.button("ğŸ” RAGå–å¾—å†…å®¹ã‚’ç¢ºèª"):
    if not input_text:
        st.error("ã¾ãšè­˜åˆ¥å­ã¾ãŸã¯URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    else:
        with st.spinner("ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’å–å¾—ä¸­..."):
            try:
                # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å–å¾—
                content_type, content = resolve_to_content(input_text)
                
                if not content:
                    st.error("ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                    st.stop()
                
                if content_type == 'text':
                    # RAGã®å‹•ä½œã‚’è©³ã—ãç¢ºèª
                    from services.summarize import text_to_documents_improved, extract_key_sections
                    from infra.vector_store import get_vector_store
                    
                    st.write("### ğŸ“„ å–å¾—ã—ãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æ¦‚è¦")
                    st.write(f"æ–‡å­—æ•°: {len(content):,} æ–‡å­—")
                    st.write(f"æœ€åˆã®200æ–‡å­—: {content[:200]}...")
                    
                    st.write("### ğŸ¯ é‡è¦ã‚»ã‚¯ã‚·ãƒ§ãƒ³æŠ½å‡ºçµæœ")
                    key_sections = extract_key_sections(content)
                    
                    for section_name, section_text in key_sections.items():
                        if section_text.strip():
                            st.write(f"**{section_name.upper()}ã‚»ã‚¯ã‚·ãƒ§ãƒ³** ({len(section_text)}æ–‡å­—):")
                            st.write(f"{section_text[:300]}...")
                            st.write("---")
                    
                    st.write("### ğŸ” RAGãƒãƒ£ãƒ³ã‚¯åˆ†å‰²çµæœ")
                    docs = text_to_documents_improved(content, key_sections)
                    st.write(f"ç·ãƒãƒ£ãƒ³ã‚¯æ•°: {len(docs)}")
                    
                    # é‡è¦åº¦é †ã«ã‚½ãƒ¼ãƒˆ
                    docs_sorted = sorted(docs, key=lambda x: x.metadata.get('importance', 1.0), reverse=True)
                    
                    for i, doc in enumerate(docs_sorted[:5]):
                        importance = doc.metadata.get('importance', 1.0)
                        section = doc.metadata.get('section', 'general')
                        st.write(f"**ãƒãƒ£ãƒ³ã‚¯ {i+1}** (é‡è¦åº¦: {importance}, ã‚»ã‚¯ã‚·ãƒ§ãƒ³: {section}):")
                        st.write(f"{doc.page_content[:400]}...")
                        st.write("---")
                    
                    st.write("### ğŸ” RAGæ¤œç´¢çµæœ")
                    vs = get_vector_store()
                    vs.add_documents(docs)
                    
                    # è¤‡æ•°ã®ã‚¯ã‚¨ãƒªã§æ¤œç´¢ãƒ†ã‚¹ãƒˆ
                    test_queries = [
                        "ç ”ç©¶ã®ç›®çš„ã¨èƒŒæ™¯",
                        "ä¸»è¦ãªçµæœã¨åŠ¹æœ",
                        "çµè«–ã¨è‡¨åºŠçš„æ„ç¾©",
                        "æ²»ç™‚åŠ¹æœã¨å®‰å…¨æ€§"
                    ]
                    
                    retriever = vs.as_retriever(
                        search_type="mmr",
                        search_kwargs={"k": 5, "fetch_k": 10, "lambda_mult": 0.7}
                    )
                    
                    for query in test_queries:
                        st.write(f"**ã‚¯ã‚¨ãƒª: '{query}'**")
                        retrieved_docs = retriever.get_relevant_documents(query)
                        
                        for j, rdoc in enumerate(retrieved_docs[:2]):
                            st.write(f"å–å¾—æ–‡æ›¸ {j+1}: {rdoc.page_content[:200]}...")
                        st.write("---")
                    
                else:
                    st.info("PDFå½¢å¼ã®ãŸã‚ã€ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå¾Œã«ãƒ‡ãƒãƒƒã‚°å¯èƒ½ã§ã™")
                    
            except Exception as e:
                import traceback
                st.error(f"ãƒ‡ãƒãƒƒã‚°å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {e}")
                st.text(traceback.format_exc())

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«çµ±è¨ˆã¨ä½¿ç”¨ä¾‹
with st.sidebar:
    st.header("ğŸ“Š å¯¾å¿œçŠ¶æ³")
    st.metric("PMCç·è¨˜äº‹æ•°", "3,454,737")
    st.metric("PDFæä¾›", "816,971 (24%)")
    st.metric("HTML/XMLå¯¾å¿œ", "ã»ã¼å…¨è¨˜äº‹")
    
    st.success("âœ… HTML/XMLå¯¾å¿œã«ã‚ˆã‚Šå¤§å¹…ã«å¯¾è±¡æ‹¡å¤§ï¼")
    
    st.header("ğŸ”¥ æ¨å¥¨ä½¿ç”¨ä¾‹")
    st.markdown("""
    **PMCIDã§è©¦ã—ã¦ã¿ã‚‹:**
    ```
    PMC12085841  (PDFç„¡ã—ã§ã‚‚OK)
    PMC5334499   (PDFæœ‰ã‚Š)
    PMC8790252   (PDFæœ‰ã‚Š)
    ```
    
    """)
    
    st.header("ğŸ’¡ å–å¾—å„ªå…ˆé †ä½")
    st.markdown("""
    1. **PDFç‰ˆ** (æœ€é«˜å“è³ª)
    2. **XMLç‰ˆ** (æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿)
    3. **HTMLç‰ˆ** (ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯)
    """)